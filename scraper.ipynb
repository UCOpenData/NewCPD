{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timestamp from string in \"mm/dd/yyyy\" format\n",
    "def maketimestamp(s):\n",
    "    #convert string into datetime format\n",
    "    date = datetime.strptime(s, \"%m/%d/%Y\")\n",
    "\n",
    "    #raise exception if date is outside range\n",
    "    today = date.today()\n",
    "    startdate = datetime.strptime(\"07/01/2010\", \"%m/%d/%Y\")\n",
    "    if date < startdate or date > today:\n",
    "        raise Exception(\"Date must be between July 1, 2010, and Today\")\n",
    "    \n",
    "    #convert datetime to timestamp and return\n",
    "    timestamp = time.mktime(date.timetuple())\n",
    "    return(int(timestamp))\n",
    "\n",
    "#create URL for incident search query from two dates\n",
    "def makeurl(start, end):\n",
    "    #if start is empty or set to \"start\", use starting date for query\n",
    "    if start == \"start\":\n",
    "        startdate = 1277960400\n",
    "    else:\n",
    "        startdate = maketimestamp(start)\n",
    "\n",
    "    #if end is empty or set to \"today\", use today for query\n",
    "    if end == \"today\":\n",
    "        enddate = int(time.mktime(date.today().timetuple()))\n",
    "    else:\n",
    "        enddate = maketimestamp(end)\n",
    "    \n",
    "    #create url from start and end dates and return\n",
    "    url = \"https://incidentreports.uchicago.edu/incidentReportArchive.php?startDate=\" \\\n",
    "        + str(startdate) + \"&endDate=\" + str(enddate)\n",
    "    return(url)\n",
    "\n",
    "#find number of pages generated by a query\n",
    "def findpagenum(url):\n",
    "    #find div with page number class\n",
    "    page = get(url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    mydivs = soup.find_all(\"li\", {\"class\": \"page-count\"})\n",
    "    pagecount = str(mydivs[0].findChildren('span'))\n",
    "\n",
    "    #clean html string and return page count\n",
    "    pagecount = pagecount.split(\"/\")[1].split(\"<\")[0]\n",
    "    return(int(pagecount))\n",
    "    \n",
    "#scrape website for all reports given an URL\n",
    "def read(url):\n",
    "    #iterate through all pages of incident query\n",
    "    for i in range(0, findpagenum(url)):\n",
    "        #change URL for each page\n",
    "        offset = i * 5\n",
    "        webpage =  url + str(offset)\n",
    "        \n",
    "        #create dataframe for first page\n",
    "        if offset == 0:\n",
    "            df = pd.read_html(webpage)[0]\n",
    "\n",
    "        #apend data from new pages to dataframe\n",
    "        else:\n",
    "            #get table data\n",
    "            try:\n",
    "                newdata = pd.read_html(webpage)[0]\n",
    "                df = df.append(newdata)\n",
    "            except:\n",
    "                print(\"Error! Broken Page: \" + website)\n",
    "    \n",
    "    #reset index and return datafram\n",
    "    df = df.reset_index().drop([\"index\"], axis=1)\n",
    "    return(df)\n",
    "\n",
    "#scrapes UChicago Incidents Page between two dates\n",
    "#primary arguement is end date, secondary arguement is starting date\n",
    "#default is starting date to today\n",
    "#dates must be in \"mm/dd/yyyy\" format\n",
    "def scrape(end = \"today\", start = \"start\"):\n",
    "    url = makeurl(start, end)\n",
    "    df = read(url)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterates through pages, and creates website link string\n",
    "df = scrape(\"08/01/2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"newoutput.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcfe9890eafba15cb7c32cab0aab5ea6f62a684385ac24c4a2ebec19cb42b6be"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
