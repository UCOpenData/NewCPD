{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#create timestamp from string in \"mm/dd/yyyy\" format\n",
    "def maketimestamp(s, page = \"police\"):\n",
    "    print(s)\n",
    "    #if input string is \"start\" or \"today\", return corresponding timestamp\n",
    "    if s == \"start\":\n",
    "        if page == \"police\": return(1277960400)\n",
    "        elif page == \"traffic\": return(1433134800)\n",
    "    elif s == \"today\":\n",
    "        return(int(time.mktime(date.today().timetuple())))\n",
    "\n",
    "    #convert string into datetime format\n",
    "    sdate = datetime.strptime(s, \"%m/%d/%Y\")\n",
    "\n",
    "    #raise exception if date is outside range\n",
    "    today = datetime.today()\n",
    "    startdate = datetime.strptime(\"07/01/2010\", \"%m/%d/%Y\")\n",
    "    if sdate < startdate or sdate > today:\n",
    "        raise Exception(\"Date must be between July 1, 2010, and Today\")\n",
    "    \n",
    "    #convert datetime to timestamp and return\n",
    "    timestamp = time.mktime(sdate.timetuple())\n",
    "    return(int(timestamp))\n",
    "\n",
    "#scrapes all UCPD incidents between two dates\n",
    "#with no dates, will scrape between start date and today\n",
    "#with one date, will scrape between start date and provided date\n",
    "#with two dates, will scrape between provided dates \n",
    "#dates must be in \"mm/dd/yyyy\" format, but order of dates does not matter\n",
    "def scrape(start, end, page):\n",
    "    #convert dates to timestamps\n",
    "    startdate = maketimestamp(start, \"page\")\n",
    "    enddate = maketimestamp(end, \"page\")\n",
    "\n",
    "    #switch start and end if in opposite order\n",
    "    if startdate > enddate:\n",
    "        (startdate, enddate) = (enddate, startdate)\n",
    "\n",
    "    #choose URL based on wheter page is police or traffic \n",
    "    if page == \"police\":\n",
    "        urlstart = \"https://incidentreports.uchicago.edu/incidentReportArchive.php?startDate=\"\n",
    "    elif page == \"traffic\":\n",
    "        urlstart = \"https://incidentreports.uchicago.edu/trafficStopsArchive.php?startDate=\"\n",
    "\n",
    "    #create url from start and end dates and return\n",
    "    url =  urlstart + str(startdate) + \"&endDate=\" + str(enddate)   \n",
    "\n",
    "    #convert to beautiful soup and find div with page count\n",
    "    page = get(url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    mydivs = soup.find_all(\"li\", {\"class\": \"page-count\"})\n",
    "\n",
    "    #convert page number html to a page num\n",
    "    pagenum = str(mydivs[0].findChildren('span'))\n",
    "    pagenum = int(pagenum.split(\"/\")[1].split(\"<\")[0])\n",
    "\n",
    "    #iterate through all pages of incident query\n",
    "    for i in range(0, pagenum):\n",
    "        #change URL for each page\n",
    "        offset = i * 5\n",
    "        webpage =  url + \"&offset=\" + str(offset)\n",
    "        \n",
    "        #create dataframe for first page\n",
    "        if offset == 0:\n",
    "            df = pd.read_html(webpage)[0]\n",
    "\n",
    "        #apend data from new pages to dataframe\n",
    "        else:\n",
    "            #get table data\n",
    "            try:\n",
    "                newdata = pd.read_html(webpage)[0]\n",
    "                df = df.append(newdata)\n",
    "            except:\n",
    "                print(\"Error! Broken Page: \" + webpage)\n",
    "                \n",
    "        #print statements for troubleshooting\n",
    "        ##print(i)\n",
    "        ##print(webpage)\n",
    "    \n",
    "    #reset index and return datafram\n",
    "    df = df.reset_index().drop([\"index\"], axis=1)\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def scrape_police(end = \"today\", start = \"start\"):\n",
    "    df = scrape(start, end, \"police\")\n",
    "    return(df)\n",
    "\n",
    "def scrape_traffic(end = \"today\", start = \"start\"):\n",
    "    df = scrape(start, end, \"traffic\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/31/2021\n",
      "06/01/2015\n"
     ]
    }
   ],
   "source": [
    "#test scrape\n",
    "df = scrape_traffic(\"06/01/2015\",\"12/31/2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1433134800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maketimestamp(\"start\", \"traffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"traffic_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcfe9890eafba15cb7c32cab0aab5ea6f62a684385ac24c4a2ebec19cb42b6be"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
