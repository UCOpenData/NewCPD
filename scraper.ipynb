{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#create timestamp from string in \"mm/dd/yyyy\" format\n",
    "def maketimestamp(s):\n",
    "    if s == \"start\":\n",
    "        return(1277960400)\n",
    "    elif s == \"today\":\n",
    "        return(int(time.mktime(date.today().timetuple())))\n",
    "    #convert string into datetime format\n",
    "    sdate = datetime.strptime(s, \"%m/%d/%Y\")\n",
    "\n",
    "    #raise exception if date is outside range\n",
    "    today = date.today()\n",
    "    startdate = datetime.strptime(\"07/01/2010\", \"%m/%d/%Y\")\n",
    "    if sdate < startdate or sdate > today:\n",
    "        raise Exception(\"Date must be between July 1, 2010, and Today\")\n",
    "    \n",
    "    #convert datetime to timestamp and return\n",
    "    timestamp = time.mktime(sdate.timetuple())\n",
    "    return(int(timestamp))\n",
    "\n",
    "#create incident search query URL from two dates\n",
    "def makeurl(start, end):\n",
    "    #convert dates to timestamps\n",
    "    startdate = maketimestamp(start)\n",
    "    enddate = maketimestamp(end)\n",
    "\n",
    "    #switch start and end if in opposite order\n",
    "    if startdate > enddate:\n",
    "        (startdate, enddate) = (enddate, startdate)\n",
    "    \n",
    "    #create url from start and end dates and return\n",
    "    url = \"https://incidentreports.uchicago.edu/incidentReportArchive.php?startDate=\" \\\n",
    "        + str(startdate) + \"&endDate=\" + str(enddate)\n",
    "    return(url)\n",
    "\n",
    "#find number of pages for a query URL\n",
    "def findpagenum(url):\n",
    "    #convert to beutifal soup and find div with page count\n",
    "    page = get(url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    mydivs = soup.find_all(\"li\", {\"class\": \"page-count\"})\n",
    "\n",
    "    #convert page number html to a page count and return\n",
    "    pagecount = str(mydivs[0].findChildren('span'))\n",
    "    pagecount = int(pagecount.split(\"/\")[1].split(\"<\")[0])\n",
    "    return(pagecount)\n",
    "    \n",
    "#scrape website for all reports for a query URL\n",
    "def read(url):\n",
    "    #iterate through all pages of incident query\n",
    "    for i in range(0, findpagenum(url)):\n",
    "        #change URL for each page\n",
    "        offset = i * 5\n",
    "        webpage =  url + \"&offset=\" + str(offset)\n",
    "        \n",
    "        #create dataframe for first page\n",
    "        if offset == 0:\n",
    "            df = pd.read_html(webpage)[0]\n",
    "\n",
    "        #apend data from new pages to dataframe\n",
    "        else:\n",
    "            #get table data\n",
    "            try:\n",
    "                newdata = pd.read_html(webpage)[0]\n",
    "                df = df.append(newdata)\n",
    "            except:\n",
    "                print(\"Error! Broken Page: \" + website)\n",
    "                \n",
    "        #print statements for troubleshooting\n",
    "        ##print(i)\n",
    "        ##print(webpage)\n",
    "    \n",
    "    #reset index and return datafram\n",
    "    df = df.reset_index().drop([\"index\"], axis=1)\n",
    "    return(df)\n",
    "\n",
    "#scrapes all UCPD incidents between two dates\n",
    "#with no dates, will scrape between start date and today\n",
    "#with one date, will scrape between start date and provided date\n",
    "#with two dates, will scrape between provided dates \n",
    "#dates must be in \"mm/dd/yyyy\" format, but order of dates does not matter\n",
    "def scrape(end = \"today\", start = \"start\"):\n",
    "    url = makeurl(start, end)   \n",
    "    df = read(url)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape(\"start\",\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"newoutput.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcfe9890eafba15cb7c32cab0aab5ea6f62a684385ac24c4a2ebec19cb42b6be"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
